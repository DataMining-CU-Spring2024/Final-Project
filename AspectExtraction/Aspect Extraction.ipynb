{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c115ff22-047e-412c-a97a-e647e68187a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-31 15:12:49 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43dfdd40e3049f5a12011a18c9d008a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-31 15:12:49 INFO: Downloaded file to /Users/sifael/stanza_resources/resources.json\n",
      "2024-05-31 15:12:50 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-05-31 15:12:50 INFO: Using device: cpu\n",
      "2024-05-31 15:12:50 INFO: Loading: tokenize\n",
      "2024-05-31 15:12:51 INFO: Loading: mwt\n",
      "2024-05-31 15:12:51 INFO: Loading: pos\n",
      "2024-05-31 15:12:51 INFO: Loading: lemma\n",
      "2024-05-31 15:12:51 INFO: Loading: constituency\n",
      "2024-05-31 15:12:51 INFO: Loading: depparse\n",
      "2024-05-31 15:12:51 INFO: Loading: sentiment\n",
      "2024-05-31 15:12:52 INFO: Loading: ner\n",
      "2024-05-31 15:12:52 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "#stanza.download('en')\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f07885b3-7893-44a6-9a5d-268d8ff7195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Initialize stop words and set global constants\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "VALID_POS_TAGS = ('JJ', 'NN', 'JJN', 'NNS', 'RB')\n",
    "\n",
    "def process_sentences(sentences, dependency_parser):\n",
    "    final_clusters = []\n",
    "    dic = {}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        print(f\"Processing sentence: {sentence}\")\n",
    "\n",
    "        # Step 1: Tokenization and POS Tagging\n",
    "        tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "        pos_tagged_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "        print(\"Step 1: Tokenization and POS Tagging completed.\")\n",
    "        print(f\"Tokenized and tagged sentence: {pos_tagged_sentence} \\n\")\n",
    "\n",
    "        # Step 2: Combining Consecutive Nouns\n",
    "        new_word_list = []\n",
    "        i = 0\n",
    "        while i < len(pos_tagged_sentence):\n",
    "            if i < len(pos_tagged_sentence) - 1 and pos_tagged_sentence[i][1] == 'NN' and pos_tagged_sentence[i+1][1] == 'NN':\n",
    "                combined_noun = pos_tagged_sentence[i][0] + pos_tagged_sentence[i+1][0]\n",
    "                new_word_list.append(combined_noun)\n",
    "                i += 2  # Skip the next item because it's already combined\n",
    "            else:\n",
    "                new_word_list.append(pos_tagged_sentence[i][0])\n",
    "                i += 1\n",
    "        print(\"Step 2: Combining consecutive nouns completed.\")\n",
    "        print(f\"New word list after combining nouns: {new_word_list} \\n\")\n",
    "\n",
    "        # Step 3: Filtering Stop Words\n",
    "        filtered_words = [word for word in new_word_list if word.lower() not in STOP_WORDS]\n",
    "        print(\"Step 3: Filtering stop words completed.\")\n",
    "        print(f\"Words after stop word filtering: {filtered_words} \\n\")\n",
    "\n",
    "        # Step 4: Dependency Parsing\n",
    "        filtered_sentence = ' '.join(filtered_words)\n",
    "        doc = dependency_parser(filtered_sentence)\n",
    "        dependencies = []\n",
    "        for dep_edge in doc.sentences[0].dependencies:\n",
    "            dependencies.append((dep_edge[2].text, dep_edge[0].id, dep_edge[1]))\n",
    "        print(\"Step 4: Dependency parsing completed.\")\n",
    "        print(f\"Dependency parse results: {dependencies} \\n\")\n",
    "\n",
    "        # Step 5: Extracting Features Based on Dependencies\n",
    "        features = [word for word in pos_tagged_sentence if word[1] in VALID_POS_TAGS]\n",
    "        feature_clusters = {}\n",
    "        for word, pos_tag in features:\n",
    "            if word in filtered_words:\n",
    "                feature_clusters[word] = pos_tag\n",
    "        print(\"Step 5: Extracting and clustering features based on dependencies completed.\")\n",
    "        print(f\"Feature clusters: {feature_clusters} \\n\")\n",
    "\n",
    "        # Step 6: Building Output Structures\n",
    "        for word, cluster in feature_clusters.items():\n",
    "            if cluster == 'NN':\n",
    "                final_clusters.append(word)\n",
    "        print(\"Step 6: Building output structures completed.\")\n",
    "        print(f\"Final clusters: {final_clusters} \\n\")\n",
    "\n",
    "    return final_clusters\n",
    "\n",
    "# Example usage (assumes dependency_parser is previously defined and loaded)\n",
    "# sentences = [\"The quick brown fox jumps over the lazy dog\", \"NLP is fun and useful\"]\n",
    "# process_sentences(sentences, dependency_parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742e436e-5a2b-46a0-a435-9bd6d98867d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe hotel service was fast and efficient. \\nWe got checked in in under 5 minutes and had all our questions answered. \\nHowever the food was terrible. Breakfast ran from 7-9 am and there was little variety. \\nThe wifi was also not very fast and we couldn't stream our favorite shows on TV. \\nOverall the experience was ok.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The hotel service was fast and efficient. \n",
    "We got checked in in under 5 minutes and had all our questions answered. \n",
    "However the food was terrible. Breakfast ran from 7-9 am and there was little variety. \n",
    "The wifi was also not very fast and we couldn't stream our favorite shows on TV. \n",
    "Overall the experience was ok.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a73718ec-8c35-4d8d-82fc-4428487528e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence: However the food was terrible. Breakfast ran from 7-9 am and there was little variety. \n",
      "Step 1: Tokenization and POS Tagging completed.\n",
      "Tokenized and tagged sentence: [('However', 'RB'), ('the', 'DT'), ('food', 'NN'), ('was', 'VBD'), ('terrible', 'JJ'), ('.', '.'), ('Breakfast', 'NNP'), ('ran', 'VBD'), ('from', 'IN'), ('7-9', 'JJ'), ('am', 'VBP'), ('and', 'CC'), ('there', 'EX'), ('was', 'VBD'), ('little', 'JJ'), ('variety', 'NN'), ('.', '.')] \n",
      "\n",
      "Step 2: Combining consecutive nouns completed.\n",
      "New word list after combining nouns: ['However', 'the', 'food', 'was', 'terrible', '.', 'Breakfast', 'ran', 'from', '7-9', 'am', 'and', 'there', 'was', 'little', 'variety', '.'] \n",
      "\n",
      "Step 3: Filtering stop words completed.\n",
      "Words after stop word filtering: ['However', 'food', 'terrible', '.', 'Breakfast', 'ran', '7-9', 'little', 'variety', '.'] \n",
      "\n",
      "Step 4: Dependency parsing completed.\n",
      "Dependency parse results: [('However', 2, 'advmod'), ('food', 0, 'root'), ('terrible', 2, 'amod'), ('.', 2, 'punct')] \n",
      "\n",
      "Step 5: Extracting and clustering features based on dependencies completed.\n",
      "Feature clusters: {'However': 'RB', 'food': 'NN', 'terrible': 'JJ', '7-9': 'JJ', 'little': 'JJ', 'variety': 'NN'} \n",
      "\n",
      "Step 6: Building output structures completed.\n",
      "Final clusters: ['food', 'variety'] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['food', 'variety']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"However the food was terrible. Breakfast ran from 7-9 am and there was little variety. \"]\n",
    "process_sentences(sentences, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a66e902-c447-41bf-ac23-c70af495ed20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
