{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "457e17da-9e3f-4dbb-9c09-f0de930d589c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 17:02:49 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0633d669c5e4274b6b2d768c1040608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-12 17:02:49 INFO: Downloaded file to /Users/sifael/stanza_resources/resources.json\n",
      "2024-05-12 17:02:50 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-05-12 17:02:50 INFO: Using device: cpu\n",
      "2024-05-12 17:02:50 INFO: Loading: tokenize\n",
      "2024-05-12 17:02:51 INFO: Loading: mwt\n",
      "2024-05-12 17:02:51 INFO: Loading: pos\n",
      "2024-05-12 17:02:51 INFO: Loading: lemma\n",
      "2024-05-12 17:02:51 INFO: Loading: constituency\n",
      "2024-05-12 17:02:51 INFO: Loading: depparse\n",
      "2024-05-12 17:02:51 INFO: Loading: sentiment\n",
      "2024-05-12 17:02:51 INFO: Loading: ner\n",
      "2024-05-12 17:02:52 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza \n",
    "\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f8f6a6-203c-4c12-abe1-a3109336dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Initialize stop words and set global constants\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "VALID_POS_TAGS = ('JJ', 'NN', 'JJN', 'NNS', 'RB')\n",
    "\n",
    "def process_sentences(sentences, dependency_parser):\n",
    "    final_clusters = []\n",
    "    dic = {}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        print(f\"Processing sentence: {sentence}\")\n",
    "\n",
    "        # Step 1: Tokenization and POS Tagging\n",
    "        tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "        pos_tagged_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "        print(\"Step 1: Tokenization and POS Tagging completed.\")\n",
    "        print(f\"Tokenized and tagged sentence: {pos_tagged_sentence} \\n\")\n",
    "\n",
    "        # Step 2: Combining Consecutive Nouns\n",
    "        new_word_list = []\n",
    "        i = 0\n",
    "        while i < len(pos_tagged_sentence):\n",
    "            if i < len(pos_tagged_sentence) - 1 and pos_tagged_sentence[i][1] == 'NN' and pos_tagged_sentence[i+1][1] == 'NN':\n",
    "                combined_noun = pos_tagged_sentence[i][0] + pos_tagged_sentence[i+1][0]\n",
    "                new_word_list.append(combined_noun)\n",
    "                i += 2  # Skip the next item because it's already combined\n",
    "            else:\n",
    "                new_word_list.append(pos_tagged_sentence[i][0])\n",
    "                i += 1\n",
    "        print(\"Step 2: Combining consecutive nouns completed.\")\n",
    "        print(f\"New word list after combining nouns: {new_word_list} \\n\")\n",
    "\n",
    "        # Step 3: Filtering Stop Words\n",
    "        filtered_words = [word for word in new_word_list if word.lower() not in STOP_WORDS]\n",
    "        print(\"Step 3: Filtering stop words completed.\")\n",
    "        print(f\"Words after stop word filtering: {filtered_words} \\n\")\n",
    "\n",
    "        # Step 4: Dependency Parsing\n",
    "        filtered_sentence = ' '.join(filtered_words)\n",
    "        doc = dependency_parser(filtered_sentence)\n",
    "        dependencies = []\n",
    "        for dep_edge in doc.sentences[0].dependencies:\n",
    "            dependencies.append((dep_edge[2].text, dep_edge[0].id, dep_edge[1]))\n",
    "        print(\"Step 4: Dependency parsing completed.\")\n",
    "        print(f\"Dependency parse results: {dependencies} \\n\")\n",
    "\n",
    "        # Step 5: Extracting Features Based on Dependencies\n",
    "        features = [word for word in pos_tagged_sentence if word[1] in VALID_POS_TAGS]\n",
    "        feature_clusters = {}\n",
    "        for word, pos_tag in features:\n",
    "            if word in filtered_words:\n",
    "                feature_clusters[word] = pos_tag\n",
    "        print(\"Step 5: Extracting and clustering features based on dependencies completed.\")\n",
    "        print(f\"Feature clusters: {feature_clusters} \\n\")\n",
    "\n",
    "        # Step 6: Building Output Structures\n",
    "        for word, cluster in feature_clusters.items():\n",
    "            if cluster == 'NN':\n",
    "                final_clusters.append(word)\n",
    "        print(\"Step 6: Building output structures completed.\")\n",
    "        print(f\"Final clusters: {final_clusters} \\n\")\n",
    "\n",
    "    return final_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25c4f307-701b-4252-8231-31eae19ddf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence: The hotel service was fast and efficient. \n",
      "Step 1: Tokenization and POS Tagging completed.\n",
      "Tokenized and tagged sentence: [('The', 'DT'), ('hotel', 'NN'), ('service', 'NN'), ('was', 'VBD'), ('fast', 'RB'), ('and', 'CC'), ('efficient', 'JJ'), ('.', '.')] \n",
      "\n",
      "Step 2: Combining consecutive nouns completed.\n",
      "New word list after combining nouns: ['The', 'hotelservice', 'was', 'fast', 'and', 'efficient', '.'] \n",
      "\n",
      "Step 3: Filtering stop words completed.\n",
      "Words after stop word filtering: ['hotelservice', 'fast', 'efficient', '.'] \n",
      "\n",
      "Step 4: Dependency parsing completed.\n",
      "Dependency parse results: [('hotelservice', 3, 'nsubj'), ('fast', 3, 'advmod'), ('efficient', 0, 'root'), ('.', 3, 'punct')] \n",
      "\n",
      "Step 5: Extracting and clustering features based on dependencies completed.\n",
      "Feature clusters: {'fast': 'RB', 'efficient': 'JJ'} \n",
      "\n",
      "Step 6: Building output structures completed.\n",
      "Final clusters: [] \n",
      "\n",
      "Processing sentence: The wifi was also not very fast and we couldn't stream our favorite shows on TV.\n",
      "Step 1: Tokenization and POS Tagging completed.\n",
      "Tokenized and tagged sentence: [('The', 'DT'), ('wifi', 'NN'), ('was', 'VBD'), ('also', 'RB'), ('not', 'RB'), ('very', 'RB'), ('fast', 'RB'), ('and', 'CC'), ('we', 'PRP'), ('could', 'MD'), (\"n't\", 'RB'), ('stream', 'VB'), ('our', 'PRP$'), ('favorite', 'JJ'), ('shows', 'NNS'), ('on', 'IN'), ('TV', 'NN'), ('.', '.')] \n",
      "\n",
      "Step 2: Combining consecutive nouns completed.\n",
      "New word list after combining nouns: ['The', 'wifi', 'was', 'also', 'not', 'very', 'fast', 'and', 'we', 'could', \"n't\", 'stream', 'our', 'favorite', 'shows', 'on', 'TV', '.'] \n",
      "\n",
      "Step 3: Filtering stop words completed.\n",
      "Words after stop word filtering: ['wifi', 'also', 'fast', 'could', \"n't\", 'stream', 'favorite', 'shows', 'TV', '.'] \n",
      "\n",
      "Step 4: Dependency parsing completed.\n",
      "Dependency parse results: [('wifi', 7, 'nsubj'), ('also', 7, 'advmod'), ('fast', 7, 'advmod'), ('could', 7, 'aux'), ('n', 7, 'advmod'), (\"'t\", 7, 'advmod'), ('stream', 0, 'root'), ('favorite', 9, 'amod'), ('shows', 10, 'compound'), ('TV', 7, 'obj'), ('.', 7, 'punct')] \n",
      "\n",
      "Step 5: Extracting and clustering features based on dependencies completed.\n",
      "Feature clusters: {'wifi': 'NN', 'also': 'RB', 'fast': 'RB', \"n't\": 'RB', 'favorite': 'JJ', 'shows': 'NNS', 'TV': 'NN'} \n",
      "\n",
      "Step 6: Building output structures completed.\n",
      "Final clusters: ['wifi', 'TV'] \n",
      "\n",
      "Processing sentence: However the food was terrible. Breakfast ran from 7-9 am and there was little variety.\n",
      "Step 1: Tokenization and POS Tagging completed.\n",
      "Tokenized and tagged sentence: [('However', 'RB'), ('the', 'DT'), ('food', 'NN'), ('was', 'VBD'), ('terrible', 'JJ'), ('.', '.'), ('Breakfast', 'NNP'), ('ran', 'VBD'), ('from', 'IN'), ('7-9', 'JJ'), ('am', 'VBP'), ('and', 'CC'), ('there', 'EX'), ('was', 'VBD'), ('little', 'JJ'), ('variety', 'NN'), ('.', '.')] \n",
      "\n",
      "Step 2: Combining consecutive nouns completed.\n",
      "New word list after combining nouns: ['However', 'the', 'food', 'was', 'terrible', '.', 'Breakfast', 'ran', 'from', '7-9', 'am', 'and', 'there', 'was', 'little', 'variety', '.'] \n",
      "\n",
      "Step 3: Filtering stop words completed.\n",
      "Words after stop word filtering: ['However', 'food', 'terrible', '.', 'Breakfast', 'ran', '7-9', 'little', 'variety', '.'] \n",
      "\n",
      "Step 4: Dependency parsing completed.\n",
      "Dependency parse results: [('However', 2, 'advmod'), ('food', 0, 'root'), ('terrible', 2, 'amod'), ('.', 2, 'punct')] \n",
      "\n",
      "Step 5: Extracting and clustering features based on dependencies completed.\n",
      "Feature clusters: {'However': 'RB', 'food': 'NN', 'terrible': 'JJ', '7-9': 'JJ', 'little': 'JJ', 'variety': 'NN'} \n",
      "\n",
      "Step 6: Building output structures completed.\n",
      "Final clusters: ['wifi', 'TV', 'food', 'variety'] \n",
      "\n",
      "Processing sentence: The wifi was also not very fast and we couldn't stream our favorite shows on TV. \n",
      "Step 1: Tokenization and POS Tagging completed.\n",
      "Tokenized and tagged sentence: [('The', 'DT'), ('wifi', 'NN'), ('was', 'VBD'), ('also', 'RB'), ('not', 'RB'), ('very', 'RB'), ('fast', 'RB'), ('and', 'CC'), ('we', 'PRP'), ('could', 'MD'), (\"n't\", 'RB'), ('stream', 'VB'), ('our', 'PRP$'), ('favorite', 'JJ'), ('shows', 'NNS'), ('on', 'IN'), ('TV', 'NN'), ('.', '.')] \n",
      "\n",
      "Step 2: Combining consecutive nouns completed.\n",
      "New word list after combining nouns: ['The', 'wifi', 'was', 'also', 'not', 'very', 'fast', 'and', 'we', 'could', \"n't\", 'stream', 'our', 'favorite', 'shows', 'on', 'TV', '.'] \n",
      "\n",
      "Step 3: Filtering stop words completed.\n",
      "Words after stop word filtering: ['wifi', 'also', 'fast', 'could', \"n't\", 'stream', 'favorite', 'shows', 'TV', '.'] \n",
      "\n",
      "Step 4: Dependency parsing completed.\n",
      "Dependency parse results: [('wifi', 7, 'nsubj'), ('also', 7, 'advmod'), ('fast', 7, 'advmod'), ('could', 7, 'aux'), ('n', 7, 'advmod'), (\"'t\", 7, 'advmod'), ('stream', 0, 'root'), ('favorite', 9, 'amod'), ('shows', 10, 'compound'), ('TV', 7, 'obj'), ('.', 7, 'punct')] \n",
      "\n",
      "Step 5: Extracting and clustering features based on dependencies completed.\n",
      "Feature clusters: {'wifi': 'NN', 'also': 'RB', 'fast': 'RB', \"n't\": 'RB', 'favorite': 'JJ', 'shows': 'NNS', 'TV': 'NN'} \n",
      "\n",
      "Step 6: Building output structures completed.\n",
      "Final clusters: ['wifi', 'TV', 'food', 'variety', 'wifi', 'TV'] \n",
      "\n",
      "Processing sentence: Overall the experience was ok.\n",
      "Step 1: Tokenization and POS Tagging completed.\n",
      "Tokenized and tagged sentence: [('Overall', 'JJ'), ('the', 'DT'), ('experience', 'NN'), ('was', 'VBD'), ('ok', 'JJ'), ('.', '.')] \n",
      "\n",
      "Step 2: Combining consecutive nouns completed.\n",
      "New word list after combining nouns: ['Overall', 'the', 'experience', 'was', 'ok', '.'] \n",
      "\n",
      "Step 3: Filtering stop words completed.\n",
      "Words after stop word filtering: ['Overall', 'experience', 'ok', '.'] \n",
      "\n",
      "Step 4: Dependency parsing completed.\n",
      "Dependency parse results: [('Overall', 2, 'amod'), ('experience', 0, 'root'), ('ok', 2, 'amod'), ('.', 2, 'punct')] \n",
      "\n",
      "Step 5: Extracting and clustering features based on dependencies completed.\n",
      "Feature clusters: {'Overall': 'JJ', 'experience': 'NN', 'ok': 'JJ'} \n",
      "\n",
      "Step 6: Building output structures completed.\n",
      "Final clusters: ['wifi', 'TV', 'food', 'variety', 'wifi', 'TV', 'experience'] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['wifi', 'TV', 'food', 'variety', 'wifi', 'TV', 'experience']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The hotel service was fast and efficient. \n",
    "We got checked in in under 5 minutes and had all our questions answered. \n",
    "However the food was terrible. Breakfast ran from 7-9 am and there was little variety. \n",
    "The wifi was also not very fast and we couldn't stream our favorite shows on TV. \n",
    "Overall the experience was ok.\"\"\"\n",
    "\n",
    "sentences = [\"The hotel service was fast and efficient. \",\n",
    "             \"The wifi was also not very fast and we couldn't stream our favorite shows on TV.\",\n",
    "            \"However the food was terrible. Breakfast ran from 7-9 am and there was little variety.\",\n",
    "             \"The wifi was also not very fast and we couldn't stream our favorite shows on TV. \",\n",
    "             \"Overall the experience was ok.\"\n",
    "            ]\n",
    "process_sentences(sentences, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e28a5b9-cbfe-41cd-94b7-4fcb3844b1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
