{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d59fba-ed3b-46cf-9b18-d3e60682f644",
   "metadata": {},
   "source": [
    "### Aspect Extraction Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b6b056-5f1c-495c-b00b-e35b003c873b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f566fae9bbe14b349db0233053e370b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 21:35:42 INFO: Downloaded file to /Users/sifael/stanza_resources/resources.json\n",
      "2024-05-07 21:35:42 INFO: Downloading default packages for language: en (English) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de43280fc02640c6a9eb8f3b9360cfda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.8.0/models/default.zip:   0%|          | 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 21:36:30 INFO: Downloaded file to /Users/sifael/stanza_resources/en/default.zip\n",
      "2024-05-07 21:36:32 INFO: Finished downloading models and saved to /Users/sifael/stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f816f20f-a9ab-4384-8831-3dd36142e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55496867-30e0-4ca2-96d4-4bc97bd74475",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "VALID_POS_TAGS = ('JJ', 'NN', 'JJR', 'NNS', 'RB')\n",
    "RELEVANT_DEPENDENCIES = {'nsubj', 'acl:relcl', 'obj', 'dobj', 'agent', 'advmod', 'amod', 'neg', 'prep_of', 'acomp', 'xcomp', 'compound'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b5a6e50-65ca-4f7e-90c9-12486177f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_nouns(tagged_list):\n",
    "    new_word_list = []\n",
    "    skip = False\n",
    "\n",
    "    for i in range(len(tagged_list) - 1):\n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "\n",
    "        current_word, current_tag = tagged_list[i]\n",
    "        next_word, next_tag = tagged_list[i + 1]\n",
    "\n",
    "        if current_tag == 'NN' and next_tag == 'NN':\n",
    "            new_word_list.append(current_word + '_' + next_word)\n",
    "            skip = True\n",
    "        else:\n",
    "            new_word_list.append(current_word)\n",
    "\n",
    "    if not skip and len(tagged_list) > 0:\n",
    "        new_word_list.append(tagged_list[-1][0])\n",
    "\n",
    "    return new_word_list\n",
    "\n",
    "\n",
    "def filter_and_retag(text):\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    filtered_words = [w for w  in word_tokens if w not in STOP_WORDS ]\n",
    "    return nltk.pos_tag(filtered_words)\n",
    "\n",
    "\n",
    "def extract_dependencies(doc, word_list):\n",
    "    dependencies = []\n",
    "    for dep_edge in doc.sentences[0].dependencies:\n",
    "        word_id = int(dep_edge[0].id) - 2\n",
    "        if word_id >= 0:\n",
    "            head_word = word_list[word_id]\n",
    "        else:\n",
    "            head_word = 'ROOT'\n",
    "        dependencies.append( [dep_edge[2].text, head_word, dep_edge[1]] )\n",
    "    return dependencies\n",
    "\n",
    "def build_clusters(features, dependencies):\n",
    "    clusters = []\n",
    "    for feature in features:\n",
    "        related = [ \n",
    "            dep[1] if dep[0] == feature[0] else dep[0]\n",
    "            for dep in dependencies\n",
    "            if dep[0] == feature[0] and dep[2] in RELEVANT_DEPENDENCIES\n",
    "        ]\n",
    "        clusters.append([feature[0], related])\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def process_sentences(sentences, dependency_parser):\n",
    "    fclusters, final_clusters, dic = [], [], {}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenize and POS tag the sentence\n",
    "        tokenized_sentence = nltk.word_tokenize(sentence)\n",
    "        pos_tagged_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "\n",
    "        # Combine consecutive nouns and for a new word list\n",
    "        combined_noun_list = combine_nouns(pos_tagged_sentence)\n",
    "        final_text = ' '.join(combined_noun_list)\n",
    "\n",
    "        # Filter Stops Words and retag the combined word list\n",
    "        filtered_tagged_list = filter_and_retag(final_text)\n",
    "\n",
    "        # Parse dependencies\n",
    "        doc = dependency_parser(final_text)\n",
    "        dependencies = extract_dependencies(doc, combined_noun_list)\n",
    "\n",
    "        # Extract Features\n",
    "        features = [word for word in filtered_tagged_list if word[1] in VALID_POS_TAGS]\n",
    "\n",
    "        # Build Clusters\n",
    "        clusters = build_clusters(features, dependencies)\n",
    "        fclusters.extend(clusters)\n",
    "\n",
    "        # Populate dictionary and final clusters\n",
    "        for word, pos_tag in features:\n",
    "            dic[word] = pos_tag\n",
    "\n",
    "        final_clusters.extend([cluster for cluster in clusters if dic[cluster[0]] == 'NN'])\n",
    "\n",
    "    return final_clusters\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89358dfb-49b2-4cc7-baf7-92112c003af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 22:27:48 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2024-05-07 22:27:48 INFO: Using device: cpu\n",
      "2024-05-07 22:27:48 INFO: Loading: tokenize\n",
      "2024-05-07 22:27:48 INFO: Loading: mwt\n",
      "2024-05-07 22:27:48 INFO: Loading: pos\n",
      "2024-05-07 22:27:48 INFO: Loading: lemma\n",
      "2024-05-07 22:27:48 INFO: Loading: constituency\n",
      "2024-05-07 22:27:48 INFO: Loading: depparse\n",
      "2024-05-07 22:27:48 INFO: Loading: sentiment\n",
      "2024-05-07 22:27:48 INFO: Loading: ner\n",
      "2024-05-07 22:27:49 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "nlp = stanza.Pipeline('en', download_method=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9f08840-efa0-4921-86bb-3282643628ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nThe hotel service was fast and efficient.',\n",
       " 'We got checked in in under 5 minutes and had all our questions answered.',\n",
       " 'However the food was terrible.',\n",
       " 'Breakfast ran from 7-9 am and there was little variety.',\n",
       " \"The wifi was also not very fast and we couldn't stream our favorite shows on TV.\",\n",
       " 'Overall the experience was ok.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "The hotel service was fast and efficient. \n",
    "We got checked in in under 5 minutes and had all our questions answered.\n",
    "However the food was terrible. \n",
    "Breakfast ran from 7-9 am and there was little variety. \n",
    "The wifi was also not very fast and we couldn't stream our favorite shows on TV. \n",
    "Overall the experience was ok.\n",
    "\"\"\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(sample_text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e74a5f14-4649-426d-a0a7-a6d1ada81706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hotel_service', ['was']],\n",
       " ['food', ['was']],\n",
       " ['variety', ['little']],\n",
       " ['wifi', ['very']],\n",
       " ['TV', []],\n",
       " ['experience', ['was']],\n",
       " ['ok', []]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "process_sentences(['\\nThe hotel service was fast and efficient.',\n",
    " 'We got checked in in under 5 minutes and had all our questions answered.',\n",
    " 'However the food was terrible.',\n",
    " 'Breakfast ran from 7-9 am and there was little variety.',\n",
    " \"The wifi was also not very fast and we couldn't stream our favorite shows on TV.\",\n",
    " 'Overall the experience was ok.'], nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd35213-4cf2-4880-9c07-53f594c413de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf6eb97-037a-4d6f-82f7-55f231df168c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
